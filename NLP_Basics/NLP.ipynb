{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9986b8-6468-4caf-bccf-8be4f457d1ee",
   "metadata": {},
   "source": [
    "# What is spaCy?\n",
    "Spacy is an Open Source Natural Landguage Processing Library designed to effectively handle NLP tasks with the most efficient implementation of common algorithm. For many NLP tasks, Spacy only has one implemented method, chosing the most efficient algorithm currently available, meaning we dont have the option to choose other algorithms.\n",
    "\n",
    "# What is NLTK?\n",
    "Natural Language Toolkit is a popular open source that provides many functionalities, but includes less efficient implementations. \n",
    "\n",
    "# NLTK vs spaCy\n",
    "Spacy does not include pre-created models for some applications, such as sentimental analysis, which is typically easier to perform with NLTK.\n",
    "\n",
    "# What is NLP?\n",
    "Natural Language Processing is an area of computer science and AI concerned with interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec18aa-2c70-4f5e-8d41-cfead79223a0",
   "metadata": {},
   "source": [
    "# spaCy Basics\n",
    "The nlp() function from spacy automatically takes raw text and performs a series of operations to tag, parse, and describe the text data.\n",
    "\r\n",
    "# spaCy Object\r\n",
    "\r\n",
    "After importing the spacy module in the cell above we loaded a **model** and named it `nlp`.<br>Next we created a **Doc** object by applying the model to our text, and named it `doc`.<br>spaCy also builds a companion **Vocab** object that we'll cover in later sections.<br>The **Doc** object that holds the processed text is our focus here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39756e4d-30fb-4389-a16f-563c5d29fb19",
   "metadata": {},
   "source": [
    "For a full list of POS Tags visit https://spacy.io/api/annotation#pos-tagging           \n",
    "For a full list of Syntactic Dependencies visit https://spacy.io/api/annotation#dependency-parsing\n",
    "To see the full name of a tag use `spacy.explain(tag)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade4f552-c866-458c-9a7a-f892ffa13002",
   "metadata": {},
   "source": [
    "|Tag|Description|doc2[0].tag|\r\n",
    "|:------|:------:|:------|\r\n",
    "|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Tesla`|\r\n",
    "|`.lemma_`|The base form of the word|`tesla`|\r\n",
    "|`.pos_`|The simple part-of-speech tag|`PROPN`/`proper noun`|\r\n",
    "|`.tag_`|The detailed part-of-speech tag|`NNP`/`noun, proper singular`|\r\n",
    "|`.shape_`|The word shape – capitalization, punctuation, digits|`Xxxxx`|\r\n",
    "|`.is_alpha`|Is the token an alpha character?|`True`|\r\n",
    "|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6053f6b6-4075-4763-ada0-a6b939e67a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla\n",
      "PROPN\n",
      "nsubj\n",
      "\n",
      "is\n",
      "AUX\n",
      "aux\n",
      "\n",
      "looking\n",
      "VERB\n",
      "ROOT\n",
      "\n",
      "at\n",
      "ADP\n",
      "prep\n",
      "\n",
      "buying\n",
      "VERB\n",
      "pcomp\n",
      "\n",
      "U.S.\n",
      "PROPN\n",
      "compound\n",
      "\n",
      "startups\n",
      "NOUN\n",
      "dobj\n",
      "\n",
      "for\n",
      "ADP\n",
      "prep\n",
      "\n",
      "$\n",
      "SYM\n",
      "quantmod\n",
      "\n",
      "6\n",
      "NUM\n",
      "compound\n",
      "\n",
      "million\n",
      "NUM\n",
      "pobj\n",
      "\n",
      ".\n",
      "PUNCT\n",
      "punct\n",
      "\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000029F51EE3C50>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x0000029F51EE3F50>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000029F51DF2110>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000029F51FDE550>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x0000029F520A8450>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000029F51DF22D0>)] \n",
      "\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "##Load language library\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc=nlp(u'Tesla is looking at buying U.S. startups for $6 million.')\n",
    "for token in doc:\n",
    "    print(token.text)##the text\n",
    "    print(token.pos_)##part of speech\n",
    "    print(token.dep_)##syntactic dependency\n",
    "    print()\n",
    "print(nlp.pipeline,'\\n')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7560f7d8-fa8b-4f96-98f5-fefc0b8d376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla \t PROPN \t nsubj \t Tesla \t Xxxxx \t NNP \t noun, proper singular \t True \t False\n",
      "is \t AUX \t aux \t be \t xx \t VBZ \t verb, 3rd person singular present \t True \t True\n",
      "n't \t PART \t neg \t not \t x'x \t RB \t adverb \t False \t True\n",
      "looking \t VERB \t ROOT \t look \t xxxx \t VBG \t verb, gerund or present participle \t True \t False\n",
      "for \t ADP \t prep \t for \t xxx \t IN \t conjunction, subordinating or preposition \t True \t True\n",
      "anymore \t ADJ \t amod \t anymore \t xxxx \t JJ \t adjective (English), other noun-modifier (Chinese) \t True \t False\n",
      "companies \t NOUN \t pobj \t company \t xxxx \t NNS \t noun, plural \t True \t False\n",
      "\n",
      "Tesla \t PROPN \t nsubj \t Tesla \t Xxxxx \t NNP \t noun, proper singular \t True \t False\n",
      "is \t AUX \t aux \t be \t xx \t VBZ \t verb, 3rd person singular present \t True \t True\n",
      "looking \t VERB \t ROOT \t look \t xxxx \t VBG \t verb, gerund or present participle \t True \t False\n",
      "at \t ADP \t prep \t at \t xx \t IN \t conjunction, subordinating or preposition \t True \t True\n",
      "buying \t VERB \t pcomp \t buy \t xxxx \t VBG \t verb, gerund or present participle \t True \t False\n",
      "U.S. \t PROPN \t compound \t U.S. \t X.X. \t NNP \t noun, proper singular \t False \t False\n",
      "startups \t NOUN \t dobj \t startup \t xxxx \t NNS \t noun, plural \t True \t False\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(u\"Tesla isn't looking for anymore companies\")\n",
    "for token in doc2:\n",
    "    print(token.text,'\\t',token.pos_,'\\t',token.dep_,'\\t',token.lemma_,'\\t',token.shape_,'\\t',token.tag_,'\\t',spacy.explain(token.tag_),'\\t',token.is_alpha,'\\t',token.is_stop)\n",
    "print()\n",
    "for i in range(len(doc2)):\n",
    "    print(doc[i].text,'\\t',doc[i].pos_,'\\t',doc[i].dep_,'\\t',doc[i].lemma_,'\\t',doc[i].shape_,'\\t',doc[i].tag_,'\\t',spacy.explain(doc[i].tag_),'\\t',doc[i].is_alpha,'\\t',doc[i].is_stop)\n",
    "print()\n",
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fcba6f-2294-40a5-8c00-2883ea3d2ca2",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking up the original text into component pieces(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c9c075-3067-4758-9437-258a2b003f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We're moving to N.C.R.!\"\n",
      "\" \t PUNCT\n",
      "We \t PRON\n",
      "'re \t AUX\n",
      "moving \t VERB\n",
      "to \t ADP\n",
      "N.C.R. \t PROPN\n",
      "! \t PUNCT\n",
      "\" \t PUNCT\n",
      "\n",
      "\n",
      "We \t PRON\n",
      "'re \t AUX\n",
      "here \t ADV\n",
      "to \t AUX\n",
      "help1 \t PROPN\n",
      "zsned \t VERB\n",
      "snail \t NOUN\n",
      "- \t PUNCT\n",
      "mail \t NOUN\n",
      ", \t PUNCT\n",
      "email \t NOUN\n",
      "support@oursite.com \t X\n",
      "or \t CCONJ\n",
      "visit \t VERB\n",
      "us \t PRON\n",
      "at \t ADP\n",
      "http://www.oursite.com \t X\n",
      "! \t PUNCT\n",
      "\n",
      "\n",
      "A \t DET\n",
      "5 \t NUM\n",
      "km \t NOUN\n",
      "NYC \t PROPN\n",
      "cab \t NOUN\n",
      "ride \t NOUN\n",
      "from \t ADP\n",
      "St. \t PROPN\n",
      "Joseph \t PROPN\n",
      "costs \t VERB\n",
      "Rs \t NOUN\n",
      "50.00 \t NUM\n",
      "/- \t PUNCT\n",
      "\n",
      "Length of doc 1 8 \n",
      "Length of doc 2 18 \n",
      "Length of doc 3 13\n"
     ]
    }
   ],
   "source": [
    "mystring = '\"We\\'re moving to N.C.R.!\"'\n",
    "print(mystring)\n",
    "doc1 = nlp(mystring)\n",
    "for token in doc1:\n",
    "    print(token.text,'\\t',token.pos_)\n",
    "print('\\n')\n",
    "\n",
    "doc2 = nlp(u\"We're here to help1 zsned snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n",
    "for token in doc2:\n",
    "    print(token.text,'\\t',token.pos_)\n",
    "print('\\n')\n",
    "\n",
    "doc3 = nlp(u\"A 5km NYC cab ride from St. Joseph costs Rs 50.00 /-\")\n",
    "for token in doc3:\n",
    "    print(token.text,'\\t',token.pos_)\n",
    "\n",
    "print('\\nLength of doc 1',len(doc1),'\\nLength of doc 2',len(doc2),'\\nLength of doc 3',len(doc3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff22ec-fdd1-4695-b164-96ebdd184789",
   "metadata": {},
   "source": [
    "# Spans\n",
    "Large Doc objects can be hard to work with at times. A **span** is a slice of Doc object in the form `Doc[start:stop]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7992f296-b3e6-4ce6-95c1-e5384ea7bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Life is what happens to us while we are making other plans\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
    "cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')\n",
    "life_quote = doc[16:30]\n",
    "print(life_quote)\n",
    "type(life_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef76085-7eff-4b12-9a14-fc84900c750e",
   "metadata": {},
   "source": [
    "# Sentences\n",
    "Certain tokens inside a Doc object may also receive a \"start of sentence\" tag. While this doesn't immediately build a list of sentences, these tags enable the generation of sentence segments through `Doc.sents`. Later we'll write our own segmentation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b3b6b3-9043-4b6e-952d-fa3415334407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is the second sentence.\n",
      "This is the last sentence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'This is the first sentence. This is the second sentence. This is the last sentence.')\n",
    "for stnc in doc.sents:\n",
    "    print(stnc)\n",
    "doc[6].is_sent_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa5dbc-db9b-4e34-a59d-b3d35e6e484e",
   "metadata": {},
   "source": [
    "\n",
    "# Named Entities\r\n",
    "Going a step beyond tokens, *named entities* add another layer of context. The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the `ents` property of a `Doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6302f0fd-ad6d-47d1-ad4a-b72f84922a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "ORG \n",
      " Companies, agencies, institutions, etc. \n",
      "\n",
      "Hong Kong\n",
      "GPE \n",
      " Countries, cities, states \n",
      "\n",
      "6 million U.S. Dollars\n",
      "MONEY \n",
      " Monetary values, including unit \n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Apple to build a factory at Hong Kong for 6 million U.S. Dollars\")\n",
    "for entity in  doc.ents:\n",
    "    print(entity)\n",
    "    print(entity.label_,'\\n',str(spacy.explain(entity.label_)),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de1cd7-be2b-42c1-a8de-a260ab5c342b",
   "metadata": {},
   "source": [
    "\n",
    "# Noun Chunks\r\n",
    "Similar to `Doc.ents`, `Doc.noun_chunks` are another object property. *Noun chunks* are \"base noun phrases\" – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun – for example, in [Sheb Wooley's 1958 song](https://en.wikipedia.org/wiki/The_Purple_People_Eater), a *\"one-eyed, one-horned, flying, purple people-eater\"* would be one long noun chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b529bd84-5cde-4f6d-a4a5-f301bb0e3c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Autonomous cars shift insurance liability towards manufacturers!\")\n",
    "for chunks in doc.noun_chunks:\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee4714-4072-4981-a8b5-b3bc32a4606e",
   "metadata": {},
   "source": [
    "\n",
    "# Built-in Visualizers\r\n",
    "\r\n",
    "spaCy includes a built-in visualization tool called **displaCy**. displaCy is able to detect whether you're working in a Jupyter notebook, and will return markup that can be rendered in a cell right away. When you export your notebook, the visualizations will be included as HTML.\r\n",
    "\r\n",
    "For more info visit https://spacy.io/usage/vi\n",
    "sual\n",
    "# Visualizing the dependency paraphi\n",
    "The optional `'distance'` argument sets the distance between tokens. If the distance is made too small, text that appears beneath short arrows may become too compressed to read.cizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98634d51-1122-4c4b-a653-02b87673c696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"988e0f8f48b544baa440bb1c8f8685bc-0\" class=\"displacy\" width=\"4075\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">going</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">bild</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">factory</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">India</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">state</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">U.P.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">sum-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">total</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">4</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">million</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">U.S.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">Dollars</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-3\" stroke-width=\"2px\" d=\"M420,352.0 C420,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,354.0 L748.0,342.0 732.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-4\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,354.0 L937,342.0 953,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-5\" stroke-width=\"2px\" d=\"M770,352.0 C770,177.0 1090.0,177.0 1090.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1090.0,354.0 L1098.0,342.0 1082.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1260.0,354.0 L1268.0,342.0 1252.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1435.0,354.0 L1443.0,342.0 1427.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-8\" stroke-width=\"2px\" d=\"M770,352.0 C770,89.5 1620.0,89.5 1620.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1620.0,354.0 L1628.0,342.0 1612.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-9\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-10\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1965.0,354.0 L1973.0,342.0 1957.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-11\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2135.0,354.0 L2143.0,342.0 2127.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-12\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,264.5 2310.0,264.5 2310.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2310.0,354.0 L2318.0,342.0 2302.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-13\" stroke-width=\"2px\" d=\"M770,352.0 C770,2.0 2500.0,2.0 2500.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2500.0,354.0 L2508.0,342.0 2492.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-14\" stroke-width=\"2px\" d=\"M2695,352.0 C2695,177.0 3015.0,177.0 3015.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2695,354.0 L2687,342.0 2703,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-15\" stroke-width=\"2px\" d=\"M2870,352.0 C2870,264.5 3010.0,264.5 3010.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,354.0 L2862,342.0 2878,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-16\" stroke-width=\"2px\" d=\"M2520,352.0 C2520,89.5 3020.0,89.5 3020.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3020.0,354.0 L3028.0,342.0 3012.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-17\" stroke-width=\"2px\" d=\"M3045,352.0 C3045,264.5 3185.0,264.5 3185.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3185.0,354.0 L3193.0,342.0 3177.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-18\" stroke-width=\"2px\" d=\"M3395,352.0 C3395,264.5 3535.0,264.5 3535.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3395,354.0 L3387,342.0 3403,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-19\" stroke-width=\"2px\" d=\"M3570,352.0 C3570,177.0 3890.0,177.0 3890.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3570,354.0 L3562,342.0 3578,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-20\" stroke-width=\"2px\" d=\"M3745,352.0 C3745,264.5 3885.0,264.5 3885.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3745,354.0 L3737,342.0 3753,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-988e0f8f48b544baa440bb1c8f8685bc-0-21\" stroke-width=\"2px\" d=\"M3220,352.0 C3220,89.5 3895.0,89.5 3895.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-988e0f8f48b544baa440bb1c8f8685bc-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3895.0,354.0 L3903.0,342.0 3887.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "doc = nlp(u\"Apple is going to bild a factory in India in the state of U.P. for a sum-total of 4 million U.S. Dollars\")\n",
    "displacy.render(doc, style = 'dep', jupyter = True,options = {'distace':110})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba64349-582b-43c7-8da3-696e568c186f",
   "metadata": {},
   "source": [
    "# Visualizing the entity recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d611a02-224f-4763-9891-da7a0a1a2449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to bild a factory in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " in the state of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.P.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for a sum-total of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    4 million U.S. Dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u\"Apple is going to bild a factory in India in the state of U.P. for a sum-total of 4 million U.S. Dollars\")\n",
    "displacy.render(doc, style = 'ent', jupyter = True,options = {'distace':110})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023b162-007f-40f3-857a-29136bbba849",
   "metadata": {},
   "source": [
    "\n",
    "# Creating Visualizations Outside of Jupyter\r\n",
    "If you're using another Python IDE or writing a script, you can choose to have spaCy serve up html separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "950f3816-9dbe-403e-8884-10de308fa5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##doc = nlp(u'This is a sentence.')\n",
    "##displacy.serve(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4cad3f-fac5-45ef-acdc-2a9c5f32c5c4",
   "metadata": {},
   "source": [
    "After running the above cell, to view the dependency parse click\n",
    "\n",
    "http://127.0.0.1:5000\n",
    "\n",
    "to shut down the server return to jupyter and interrupt the Kernel either through Kernel menu, by hitting the black square on the toolbar, or by typing the keyboard shortcut `Esc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1233fe-d8b3-47d0-a6d1-6c04a403eb1f",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Often when searching text for a certain keyword, it helps if the search returns variations of the word. For instance, searching for \"boat\" might also return \"boats\" and \"boating\". Here, \"boat\" would be the **stem** for [boat, boater, boating, boats].\r\n",
    "\r\n",
    "Stemming is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached. This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required. In fact, spaCy doesn't include a stemmer, opting instead to rely entirely on lemmatization. For those interested, there's some background on this decision [here](https://github.com/explosion/spaCy/issues/327). We discuss the virtues of *lemmatization* in the next section.\r\n",
    "\r\n",
    "Instead, we'll use another popular NLP tool called **nltk**, which stands for *Natural Language Toolkit*. For more information on nltk visit https://www.nltk.\n",
    "\n",
    "# Porter Stemmer\r\n",
    "\r\n",
    "One of the most common - and effective - stemming tools is [*Porter's Algorithm*](https://tartarus.org/martin/PorterStemmer/) developed by Martin Porter in [1980](https://tartarus.org/martin/PorterStemmer/def.txt). The algorithm employs five phases of word reduction, each with its own set of mapping rules. In the first phase, simple suffix mapping rules are de.1.png)\n",
    "\n",
    "From a given set of stemming rules only one rule is applied, based on the longest suffix S1. Thus, `caresses` reduces to `caress` but not `cares`.\r\n",
    "\r\n",
    "More sophisticated phases consider the length/complexity of the word before applyie.e:s:org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "457a598e-9a9a-427d-944f-3dc81dc069d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer\n",
      "\n",
      "run---->run\n",
      "runner---->runner\n",
      "ran---->ran\n",
      "runs---->run\n",
      "easily---->easili\n",
      "fairly---->fairli\n",
      "fairness---->fair\n",
      "generously---->gener\n",
      "generate---->gener\n",
      "generous---->gener\n",
      "generation---->gener\n",
      "\n",
      "\n",
      "SnowballStemmer\n",
      "\n",
      "run---->run\n",
      "runner---->runner\n",
      "ran---->ran\n",
      "runs---->run\n",
      "easily---->easili\n",
      "fairly---->fair\n",
      "fairness---->fair\n",
      "generously---->generous\n",
      "generate---->generat\n",
      "generous---->generous\n",
      "generation---->generat\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"PorterStemmer\\n\")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "words = ['run','runner','ran','runs','easily','fairly','fairness','generously','generate','generous','generation']\n",
    "for wrd in words:\n",
    "    print(wrd + '---->' + p_stemmer.stem(wrd))\n",
    "print(\"\\n\\nSnowballStemmer\\n\")\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language = 'english')\n",
    "for wrd in words:\n",
    "    print(wrd + '---->' + s_stemmer.stem(wrd))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5173583d-e556-4c15-9473-d5e848be2e7a",
   "metadata": {},
   "source": [
    "\n",
    "Stemming has its drawbacks. If given the token `saw`, stemming might always return `saw`, whereas lemmatization would likely return either `see` or `saw` depending on whether the use of the token was as a verb or a noun. As an example, consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a00433d-7891-4029-9f59-8d282e0895dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> i\n",
      "am --> am\n",
      "meeting --> meet\n",
      "him --> him\n",
      "tomorrow --> tomorrow\n",
      "at --> at\n",
      "the --> the\n",
      "meeting --> meet\n"
     ]
    }
   ],
   "source": [
    "phrase = 'I am meeting him tomorrow at the meeting'\n",
    "for word in phrase.split():\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabd3cd-9b9c-4f40-b861-62de5199c9a2",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence.\n",
    "Lemmatization is much more informative than simple stmming which is why spaCy has opted to only have Lemmatization insteam of stemming. Lemmatization looks at the surrounding text to determine a given word's part of speech, it does not categorize phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff3fe13b-6eec-4290-a304-6790f0904895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "i \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am a runner running in a race because I love to run since i ran today')\n",
    "for token in doc:\n",
    "    print(token.text,'\\t',token.pos_,'\\t',token.lemma,'\\t',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb41c28-7fdb-401b-98a3-28db3ed78e05",
   "metadata": {},
   "source": [
    "# Function to show lemmas\n",
    "Since the above o/p is staggered we define a function that show us the lemmas in more systemic manner and more neatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c889eff-d87e-45e2-9215-57e0e57e1221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I           PRON  4690420944186131903   I\n",
      "am          AUX   10382539506755952630  be\n",
      "a           DET   11901859001352538922  a\n",
      "runner      NOUN  12640964157389618806  runner\n",
      "running     VERB  12767647472892411841  run\n",
      "in          ADP   3002984154512732771   in\n",
      "a           DET   11901859001352538922  a\n",
      "race        NOUN  8048469955494714898   race\n",
      "because     SCONJ 16950148841647037698  because\n",
      "I           PRON  4690420944186131903   I\n",
      "love        VERB  3702023516439754181   love\n",
      "to          PART  3791531372978436496   to\n",
      "run         VERB  12767647472892411841  run\n",
      "since       SCONJ 10066841407251338481  since\n",
      "I           PRON  4690420944186131903   I\n",
      "ran         VERB  12767647472892411841  run\n",
      "today       NOUN  11042482332948150395  today\n",
      "when        SCONJ 15807309897752499399  when\n",
      "I           PRON  4690420944186131903   I\n",
      "saw         VERB  11925638236994514241  see\n",
      "eighteen    NUM   9609336664675087640   eighteen\n",
      "mice        NOUN  1384165645700560590   mouse\n"
     ]
    }
   ],
   "source": [
    "def show_lemmas(doc):\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:{12}}{token.pos_:{6}}{token.lemma:<{22}}{token.lemma_:}\")\n",
    "doc = nlp(u'I am a runner running in a race because I love to run since I ran today when I saw eighteen mice')\n",
    "show_lemmas(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e644e7-8cc8-4861-bb81-a2edaba527ed",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "Words that appear more frequently and hence don't need a tag as throughly as nouns, verbs or modifiers. These words like 'a', 'an', 'the' etc are called stop words and they can be filtered from the text to be processed, spaCy holds around 326 such stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2df8ba58-11ba-46fa-9b04-71ed55ed536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'being', 'do', 'must', 'whence', 'whether', \"n't\", 'in', \"'d\", 'would', 'fifty', 'be', 'either', 'hence', 'can', 'just', 'before', 'itself', 'whom', 'became', 'until', 'front', 'whereupon', 'both', 'am', 'used', 'only', 'upon', 'hundred', 'make', 'therein', 'becomes', 'nor', '‘m', 'yourself', '’ve', '‘ll', 'on', 'name', 'below', 'under', '‘ve', 'it', 'ever', 'eleven', 'bottom', 'their', 'i', 'during', 'yourselves', 'anyone', 'indeed', 'thru', 'afterwards', 'meanwhile', 'has', 'anyway', 'could', 'seeming', 'its', 'off', 'move', 'into', 'n’t', 'none', 'amongst', '’re', 'mine', 'without', 'than', 'sometimes', 'whenever', 'down', 'again', 'will', 'hereafter', 'more', 'though', 'first', 'everyone', 'herein', 'neither', 'go', 'almost', 'then', 'thence', 'using', 'should', 'not', 'hers', 'ten', 'rather', 'seems', 'made', 'otherwise', 'beforehand', 'already', 'eight', 'that', 'does', 'thereby', 'three', 'whatever', 'for', 'never', 'become', 'about', 'among', 'all', \"'ll\", 'were', 'noone', 'unless', 'give', 'such', 'still', 'what', 'myself', 'anyhow', 'twenty', 'seemed', 'someone', 'now', 'each', 'further', 'regarding', 'cannot', 'above', 'there', 'at', 'us', 'per', 'towards', 'most', 'somehow', 'too', 'any', 'n‘t', 'by', 'yet', 'these', 'themselves', 'keep', 'he', 'same', 'whoever', '‘re', 'nine', 'whereby', 'throughout', 'no', 'often', 'we', 'to', 'latterly', 'seem', 'doing', 'take', '‘s', 'hereupon', 'thereupon', 'full', 'whose', 'around', 'nothing', 'very', \"'m\", 'was', 'as', 'from', 'the', 'out', 'also', 'enough', 'wherein', 'and', 'everything', 'thereafter', 'hereby', 'much', 'own', 'well', 'get', 'if', 'somewhere', 'quite', 'everywhere', 'others', 'former', 'really', 'across', '’m', 'part', 'say', 'five', 'ourselves', 'nevertheless', 'may', 'latter', 'alone', 'another', 'fifteen', 'becoming', 'mostly', 'besides', 'else', 'this', 'together', 'while', 'between', 'even', 'you', 'those', 'thus', 'who', 'might', 'always', 'call', 'although', 'however', 'but', 'them', 'so', 'onto', 'over', 'of', 'perhaps', 'whole', 'put', 'whereafter', '’ll', 'a', 'himself', 'many', 'within', 'his', 'since', 'whereas', 'other', 'various', 'along', 'sometime', 'me', 'several', 'via', 'beyond', 'amount', 'two', '‘d', '’s', 'side', 'four', 'less', 'please', 'except', 'our', 'whither', 'one', 'why', 'twelve', 'serious', 'toward', \"'re\", 'him', 'been', 'here', '’d', 're', 'her', 'because', 'an', 'anywhere', 'nowhere', 'once', 'is', 'did', 'something', 'every', 'how', 'wherever', 'with', 'she', 'after', \"'s\", 'last', 'some', 'through', 'see', 'sixty', 'they', 'anything', 'are', 'done', 'top', 'my', 'herself', 'ours', 'due', 'few', 'beside', 'formerly', 'back', 'your', 'show', \"'ve\", 'against', 'ca', 'therefore', 'yours', 'nobody', 'had', 'or', 'six', 'up', 'next', 'least', 'which', 'third', 'namely', 'where', 'forty', 'moreover', 'elsewhere', 'when', 'have', 'empty', 'behind'} \n",
      "\n",
      " 326\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words,'\\n\\n',len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f29973-148c-4af9-aaf0-abbd44229e8b",
   "metadata": {},
   "source": [
    "We can check whether a word is a stop word or not by using `nlp.vocab['word'].is_stop`\n",
    "\n",
    "We can also add stop words if we feel that they are used multiple times, to the default set by using `nlp.Defaults.stop_words.add('word')` floowed by `nlp.vocab['btw'].is_stop=True`\n",
    "\n",
    "We can remove a stop word by using `nlp.Defaults.stop_words.remove('word')` followed by `nlp.vocab['word'].is_stop=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c052ba5-7cd1-4405-8edf-679f0a7d63ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True \t False\n",
      "326\n",
      "\n",
      " 327\n",
      "\n",
      " 326\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab['only'].is_stop,'\\t',nlp.vocab['India'].is_stop)\n",
    "\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "nlp.vocab['btw'].is_stop = True\n",
    "print('\\n',len(nlp.Defaults.stop_words))\n",
    "\n",
    "nlp.Defaults.stop_words.remove('btw')\n",
    "nlp.vocab['btw'].is_stop = False\n",
    "print('\\n',len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18e608-fbc8-4de3-888a-a56ada9a65e8",
   "metadata": {},
   "source": [
    "# Vocabulary and Matching\n",
    "So far we have seen how a body of text is divided into tokens , and how indivisual tokens are parsed and tagged eith parts of speech, depandencies and lemmas. Now we will identify and label specific phrases that matches patterns we can define ourselves\n",
    "\n",
    "# Rule-based Matching \n",
    "spaCy offers a rule-based matching tool called `Matcher`that allows you to buikd a library of toknes patterns, then match those patterns against a doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher.\n",
    "\n",
    "\n",
    "`matcher` returns a list of tuples. Each tuple contains an ID for the match, with start & end tokens that map to the span `doc[start:end]`. The `match_id` is simply the hash value of the `string_ID` 'SolarPower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a28e0ef-5346-4be6-a37e-52c829d1ba67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{'LOWER' : 'solarpower'}]\n",
    "pattern2 = [{'LOWER' : 'solar'},{'LOWER' : 'power'}]\n",
    "pattern3 = [{'LOWER' : 'solar'},{'IS_PUNCT' : True},{'LOWER' : 'power'}]\n",
    "#pattern=[[{'LOWER' : 'solarpower'}],[{'LOWER' : 'solar'},{'LOWER' : 'power'}],[{'LOWER' : 'solar'},{'IS_PUNCT' : True},{'LOWER' : 'power'}]]\n",
    "\n",
    "matcher.add('SolarPower', [pattern1, pattern2, pattern3])\n",
    "#matcher.add('SolarPower', pattern)\n",
    "\n",
    "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
    "for solarpower increases. Solar-power cars are gaining popularity.')\n",
    "\n",
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de55a7e-9721-4a56-b0c2-45793cd35306",
   "metadata": {},
   "source": [
    "# Making a function for finding matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c57a17f-082d-458e-902a-9dc0166aacdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 10 11 solarpower\n",
      "8656102463236116519 SolarPower 13 16 Solar-power\n"
     ]
    }
   ],
   "source": [
    "def findingMatches(doc):\n",
    "    found_matches = matcher(doc)\n",
    "    for match_id, start, end in found_matches:\n",
    "        string_id = nlp.vocab.strings[match_id] # get string representation\n",
    "        span = doc[start:end]                    # get the matched span\n",
    "        print(match_id, string_id, start, end, span.text)\n",
    "        \n",
    "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
    "for solarpower increases. Solar-power cars are gaining popularity.')\n",
    "\n",
    "findingMatches(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7da76-69c5-4f26-b8a1-e7f5f1d5f4ff",
   "metadata": {},
   "source": [
    "# Setting pattern options and quantifiers\r\n",
    "You can make token rules optional by passing an `'OP':'*'` argument. This lets us streamline our patterns lis. The following codes found both two-word patterns, with and without the hyphen!\r\n",
    "\r\n",
    "The following quantifiers can be passed to the `'OP'` key:\r\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\r\n",
    "\r\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\r\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\r\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\r\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\r\n",
    "</table>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64269352-e3f7-4672-a3f3-b6e44275c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
    "#patterns = [[{'LOWER': 'solarpower'}], [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]]\n",
    "\n",
    "matcher.remove('SolarPower')\n",
    "\n",
    "matcher.add('SolarPower', [pattern1, pattern2])\n",
    "#matcher.add('SolarPower', patterns)\n",
    "\n",
    "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
    "for solarpower increases. Solar-power cars are gaining popularity.')\n",
    "\n",
    "found_matches = matcher(doc)\n",
    "print(found_matches)\n",
    "\n",
    "doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')\n",
    "found_matches_doc2 = matcher(doc2)\n",
    "print(found_matches_doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e656f-f5a7-4e35-8bcb-c1c51f57d91b",
   "metadata": {},
   "source": [
    "# Working with lemmas!\r\n",
    "If we wanted to match on both 'solar power' and 'solar powered', it might be tempting to look for the *lemma* of 'powered' and expect it to be 'power'. This is not always the case! The lemma of the *adjective* 'powered' is still 'powered:\n",
    "The matcher found the first occurrence because the lemmatizer treated 'Solar-powered' as a verb, but not the second as it considered it an adjective.<br>For this case it may be better to set explicit token patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "172542d7-6ff6-4308-b328-5f6fe9cad46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
     ]
    }
   ],
   "source": [
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
    "pattern3 = [{'LOWER': 'solarpowered'}]\n",
    "pattern4 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'powered'}]\n",
    "##patterns=[[{'LOWER': 'solarpower'}], [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}], [{'LOWER': 'solarpowered'}], [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'powered'}]}\n",
    "\n",
    "matcher.remove('SolarPower')\n",
    "\n",
    "matcher.add('SolarPower',[pattern1, pattern2, pattern3, pattern4])\n",
    "#matcher.add('SolarPower',patterns)\n",
    "\n",
    "doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')\n",
    "found_matches_doc2 = matcher(doc2)\n",
    "print(found_matches_doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51125c-842d-4e11-b63a-6e5694658b01",
   "metadata": {},
   "source": [
    "# Other token attributes\r\n",
    "Besides lemmas, there are a variety of token attributes we can use to determine matching rules:\r\n",
    "<table><tr><th>Attribute</th><th>Description</th></tr>\r\n",
    "\r\n",
    "<tr ><td><span >`ORTH`</span></td><td>The exact verbatim text of a token</td></tr>\r\n",
    "<tr ><td><span >`LOWER`</span></td><td>The lowercase form of the token text</td></tr>\r\n",
    "<tr ><td><span >`LENGTH`</span></td><td>The length of the token text</td></tr>\r\n",
    "<tr ><td><span >`IS_ALPHA`, `IS_ASCII`, `IS_DIGIT`</span></td><td>Token text consists of alphanumeric characters, ASCII characters, digits</td></tr>\r\n",
    "<tr ><td><span >`IS_LOWER`, `IS_UPPER`, `IS_TITLE`</span></td><td>Token text is in lowercase, uppercase, titlecase</td></tr>\r\n",
    "<tr ><td><span >`IS_PUNCT`, `IS_SPACE`, `IS_STOP`</span></td><td>Token is punctuation, whitespace, stop word</td></tr>\r\n",
    "<tr ><td><span >`LIKE_NUM`, `LIKE_URL`, `LIKE_EMAIL`</span></td><td>Token text resembles a number, URL, email</td></tr>\r\n",
    "<tr ><td><span >`POS`, `TAG`, `DEP`, `LEMMA`, `SHAPE`</span></td><td>The token's simple and extended part-of-speech tag, dependency label, lemma, shape</td></tr>\r\n",
    "<tr ><td><span >`ENT_TYPE`</span></td><td>The token's entity label</td></tr>\r\n",
    "\r\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c41da6-cfcd-4c4f-9450-261fe550378a",
   "metadata": {},
   "source": [
    "# Token wildcard\n",
    "You can pass an empty dictionary `{}` as a wildcard to represent*any toke*. For example, you might want to retrieve hashtags without knowing what might follow the `#` charater we can use `[{'ORTH':'#'},{}]`\n",
    "\n",
    "## PhraseMatcher\r\n",
    "In the above section we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into `matcher` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "436a8cad-794a-4226-b312-4f7ca02a8b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3473369816841043438, 41, 45),\n",
       " (3473369816841043438, 49, 53),\n",
       " (3473369816841043438, 54, 56),\n",
       " (3473369816841043438, 61, 65),\n",
       " (3473369816841043438, 673, 677),\n",
       " (3473369816841043438, 2986, 2990)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "with open('C:/nlp/textFiles/reaganomics.txt', encoding= 'unicode_escape') as f:\n",
    "    doc3 = nlp(f.read())\n",
    "\n",
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n",
    "\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "\n",
    "matcher.add('VoodooEconomics', phrase_patterns)\n",
    "\n",
    "matches = matcher(doc3)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5de500ad-a215-4823-8d12-9c0b1a4ccfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove('VoodooEconomics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c2036c-4008-45c0-8130-03f0ee6d462d",
   "metadata": {},
   "source": [
    "# Part of Speech Basics\r\n",
    "The challenge of correctly identifying parts of speech is summed up nicely in the [spaCy docs](https://spacy.io/usage/linguistic-features)\n",
    "\n",
    "Processing raw text intelligently is difficult: most words are rare, and it's common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. While it's possible to solve some problems starting from only the raw characters, it's usually better to use linguistic knowledge to add useful information. That's exactly what spaCy is designed to do: you put in raw text, and get back a **Doc** object, that comes with a variety of annotations.\n",
    "\n",
    "In this section we'll take a closer look at coarse POS tags (noun, verb, adjective) and fine-grained tags (plural noun, past-tense verb, superlative adjective.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac6fad-920e-4512-b922-ffcc13c268af",
   "metadata": {},
   "source": [
    "## View token tags\n",
    "Recall that you can obtain a particular token by its index position.\n",
    "* To view the coarse POS tag use `token.pos_`\n",
    "* To view the fine-grained tag use `token.tag_`\n",
    "* To view the description of either type of tag use `spacy.explain(tag)`\n",
    "\n",
    "Note that `token.pos` and `token.tag` return integer hash values; by adding the underscore we get the text equivalent that lives in **doc.vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1341207c-6cef-4896-b355-882c0ecf5231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \t DET \t DT \t determiner\n",
      "quick \t ADJ \t JJ \t adjective (English), other noun-modifier (Chinese)\n",
      "brown \t ADJ \t JJ \t adjective (English), other noun-modifier (Chinese)\n",
      "fox \t NOUN \t NN \t noun, singular or mass\n",
      "jumped \t VERB \t VBD \t verb, past tense\n",
      "over \t ADP \t IN \t conjunction, subordinating or preposition\n",
      "the \t DET \t DT \t determiner\n",
      "lazy \t ADJ \t JJ \t adjective (English), other noun-modifier (Chinese)\n",
      "dog \t NOUN \t NN \t noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'The quick brown fox jumped over the lazy dog')\n",
    "for token in doc:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.tag_, '\\t', spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62e52e-759e-4ffb-80c9-055089728e58",
   "metadata": {},
   "source": [
    "# Coarse-grained Part-of-speech Tags\r\n",
    "Every token is assigned a POS Tag from the following list:\r\n",
    "\r\n",
    "\r\n",
    "<table><tr><th>POS</th><th>DESCRIPTION</th><th>EXAMPLES</th></tr>\r\n",
    "    \r\n",
    "<tr><td>ADJ</td><td>adjective</td><td>*big, old, green, incomprehensible, first*</td></tr>\r\n",
    "<tr><td>ADP</td><td>adposition</td><td>*in, to, during*</td></tr>\r\n",
    "<tr><td>ADV</td><td>adverb</td><td>*very, tomorrow, down, where, there*</td></tr>\r\n",
    "<tr><td>AUX</td><td>auxiliary</td><td>*is, has (done), will (do), should (do)*</td></tr>\r\n",
    "<tr><td>CONJ</td><td>conjunction</td><td>*and, or, but*</td></tr>\r\n",
    "<tr><td>CCONJ</td><td>coordinating conjunction</td><td>*and, or, but*</td></tr>\r\n",
    "<tr><td>DET</td><td>determiner</td><td>*a, an, the*</td></tr>\r\n",
    "<tr><td>INTJ</td><td>interjection</td><td>*psst, ouch, bravo, hello*</td></tr>\r\n",
    "<tr><td>NOUN</td><td>noun</td><td>*girl, cat, tree, air, beauty*</td></tr>\r\n",
    "<tr><td>NUM</td><td>numeral</td><td>*1, 2017, one, seventy-seven, IV, MMXIV*</td></tr>\r\n",
    "<tr><td>PART</td><td>particle</td><td>*'s, not,*</td></tr>\r\n",
    "<tr><td>PRON</td><td>pronoun</td><td>*I, you, he, she, myself, themselves, somebody*</td></tr>\r\n",
    "<tr><td>PROPN</td><td>proper noun</td><td>*Mary, John, London, NATO, HBO*</td></tr>\r\n",
    "<tr><td>PUNCT</td><td>punctuation</td><td>*., (, ), ?*</td></tr>\r\n",
    "<tr><td>SCONJ</td><td>subordinating conjunction</td><td>*if, while, that*</td></tr>\r\n",
    "<tr><td>SYM</td><td>symbol</td><td>*$, %, §, ©, +, −, ×, ÷, =, :), 😝*</td></tr>\r\n",
    "<tr><td>VERB</td><td>verb</td><td>*run, runs, running, eat, ate, eating*</td></tr>\r\n",
    "<tr><td>X</td><td>other</td><td>*sfpksdpsxmsa*</td></tr>\r\n",
    "<tr><td>SPACE</td><td>space</td></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d14647-4423-4d63-b5e5-83b7314f28d1",
   "metadata": {},
   "source": [
    "## Fine-grained Part-of-speech Tags\r\n",
    "Tokens are subsequently given a fine-grained tag as determined by morphology:\r\n",
    "<table>\r\n",
    "<tr><th>POS</th><th>Description</th><th>Fine-grained Tag</th><th>Description</th><th>Morphology</th></tr>\r\n",
    "<tr><td>ADJ</td><td>adjective</td><td>AFX</td><td>affix</td><td>Hyph=yes</td></tr>\r\n",
    "<tr><td>ADJ</td><td></td><td>JJ</td><td>adjective</td><td>Degree=pos</td></tr>\r\n",
    "<tr><td>ADJ</td><td></td><td>JJR</td><td>adjective, comparative</td><td>Degree=comp</td></tr>\r\n",
    "<tr><td>ADJ</td><td></td><td>JJS</td><td>adjective, superlative</td><td>Degree=sup</td></tr>\r\n",
    "<tr><td>ADJ</td><td></td><td>PDT</td><td>predeterminer</td><td>AdjType=pdt PronType=prn</td></tr>\r\n",
    "<tr><td>ADJ</td><td></td><td>PRP\\$</td><td>pronoun, possessive</td><td>PronType=prs Poss=yes</td></tr>\r\n",
    "<tr><td>ADJ</td><td></td><td>WDT</td><td>wh-determiner</td><td>PronType=int rel</td></tr>\r\n",
    "<tr><td>ADJ</td><td></td><td>WP\\$</td><td>wh-pronoun, possessive</td><td>Poss=yes PronType=int rel</td></tr>\r\n",
    "<tr><td>ADP</td><td>adposition</td><td>IN</td><td>conjunction, subordinating or preposition</td><td></td></tr>\r\n",
    "<tr><td>ADV</td><td>adverb</td><td>EX</td><td>existential there</td><td>AdvType=ex</td></tr>\r\n",
    "<tr><td>ADV</td><td></td><td>RB</td><td>adverb</td><td>Degree=pos</td></tr>\r\n",
    "<tr><td>ADV</td><td></td><td>RBR</td><td>adverb, comparative</td><td>Degree=comp</td></tr>\r\n",
    "<tr><td>ADV</td><td></td><td>RBS</td><td>adverb, superlative</td><td>Degree=sup</td></tr>\r\n",
    "<tr><td>ADV</td><td></td><td>WRB</td><td>wh-adverb</td><td>PronType=int rel</td></tr>\r\n",
    "<tr><td>CONJ</td><td>conjunction</td><td>CC</td><td>conjunction, coordinating</td><td>ConjType=coor</td></tr>\r\n",
    "<tr><td>DET</td><td>determiner</td><td>DT</td><td>determiner</td><td></td></tr>\r\n",
    "<tr><td>INTJ</td><td>interjection</td><td>UH</td><td>interjection</td><td></td></tr>\r\n",
    "<tr><td>NOUN</td><td>noun</td><td>NN</td><td>noun, singular or mass</td><td>Number=sing</td></tr>\r\n",
    "<tr><td>NOUN</td><td></td><td>NNS</td><td>noun, plural</td><td>Number=plur</td></tr>\r\n",
    "<tr><td>NOUN</td><td></td><td>WP</td><td>wh-pronoun, personal</td><td>PronType=int rel</td></tr>\r\n",
    "<tr><td>NUM</td><td>numeral</td><td>CD</td><td>cardinal number</td><td>NumType=card</td></tr>\r\n",
    "<tr><td>PART</td><td>particle</td><td>POS</td><td>possessive ending</td><td>Poss=yes</td></tr>\r\n",
    "<tr><td>PART</td><td></td><td>RP</td><td>adverb, particle</td><td></td></tr>\r\n",
    "<tr><td>PART</td><td></td><td>TO</td><td>infinitival to</td><td>PartType=inf VerbForm=inf</td></tr>\r\n",
    "<tr><td>PRON</td><td>pronoun</td><td>PRP</td><td>pronoun, personal</td><td>PronType=prs</td></tr>\r\n",
    "<tr><td>PROPN</td><td>proper noun</td><td>NNP</td><td>noun, proper singular</td><td>NounType=prop Number=sign</td></tr>\r\n",
    "<tr><td>PROPN</td><td></td><td>NNPS</td><td>noun, proper plural</td><td>NounType=prop Number=plur</td></tr>\r\n",
    "<tr><td>PUNCT</td><td>punctuation</td><td>-LRB-</td><td>left round bracket</td><td>PunctType=brck PunctSide=ini</td></tr>\r\n",
    "<tr><td>PUNCT</td><td></td><td>-RRB-</td><td>right round bracket</td><td>PunctType=brck PunctSide=fin</td></tr>\r\n",
    "<tr><td>PUNCT</td><td></td><td>,</td><td>punctuation mark, comma</td><td>PunctType=comm</td></tr>\r\n",
    "<tr><td>PUNCT</td><td></td><td>:</td><td>punctuation mark, colon or ellipsis</td><td></td></tr>\r\n",
    "<tr><td>PUNCT</td><td></td><td>.</td><td>punctuation mark, sentence closer</td><td>PunctType=peri</td></tr>\r\n",
    "<tr><td>PUNCT</td><td></td><td>''</td><td>closing quotation mark</td><td>PunctType=quot PunctSide=fin</td></tr>\r\n",
    "<tr><td>PUNCT</td><td></td><td>\"\"</td><td>closing quotation mark</td><td>PunctType=quot PunctSide=fin</td></tr>\r\n",
    "<tr><td>PUNCT</td><td></td><td>``</td><td>opening quotation mark</td><td>PunctType=quot PunctSide=ini</td></tr>\r\n",
    "<tr><td>PUNCT</td><td></td><td>HYPH</td><td>punctuation mark, hyphen</td><td>PunctType=dash</td></tr>\r\n",
    "<tr><td>PUNCT</td><td></td><td>LS</td><td>list item marker</td><td>NumType=ord</td></tr>\r\n",
    "<tr><td>PUNCT</td><td></td><td>NFP</td><td>superfluous punctuation</td><td></td></tr>\r\n",
    "<tr><td>SYM</td><td>symbol</td><td>#</td><td>symbol, number sign</td><td>SymType=numbersign</td></tr>\r\n",
    "<tr><td>SYM</td><td></td><td>\\$</td><td>symbol, currency</td><td>SymType=currency</td></tr>\r\n",
    "<tr><td>SYM</td><td></td><td>SYM</td><td>symbol</td><td></td></tr>\r\n",
    "<tr><td>VERB</td><td>verb</td><td>BES</td><td>auxiliary \"be\"</td><td></td></tr>\r\n",
    "<tr><td>VERB</td><td></td><td>HVS</td><td>forms of \"have\"</td><td></td></tr>\r\n",
    "<tr><td>VERB</td><td></td><td>MD</td><td>verb, modal auxiliary</td><td>VerbType=mod</td></tr>\r\n",
    "<tr><td>VERB</td><td></td><td>VB</td><td>verb, base form</td><td>VerbForm=inf</td></tr>\r\n",
    "<tr><td>VERB</td><td></td><td>VBD</td><td>verb, past tense</td><td>VerbForm=fin Tense=past</td></tr>\r\n",
    "<tr><td>VERB</td><td></td><td>VBG</td><td>verb, gerund or present participle</td><td>VerbForm=part Tense=pres Aspect=prog</td></tr>\r\n",
    "<tr><td>VERB</td><td></td><td>VBN</td><td>verb, past participle</td><td>VerbForm=part Tense=past Aspect=perf</td></tr>\r\n",
    "<tr><td>VERB</td><td></td><td>VBP</td><td>verb, non-3rd person singular present</td><td>VerbForm=fin Tense=pres</td></tr>\r\n",
    "<tr><td>VERB</td><td></td><td>VBZ</td><td>verb, 3rd person singular present</td><td>VerbForm=fin Tense=pres Number=sing Person=3</td></tr>\r\n",
    "<tr><td>X</td><td>other</td><td>ADD</td><td>email</td><td></td></tr>\r\n",
    "<tr><td>X</td><td></td><td>FW</td><td>foreign word</td><td>Foreign=yes</td></tr>\r\n",
    "<tr><td>X</td><td></td><td>GW</td><td>additional word in multi-word expression</td><td></td></tr>\r\n",
    "<tr><td>X</td><td></td><td>XX</td><td>unknown</td><td></td></tr>\r\n",
    "<tr><td>SPACE</td><td>space</td><td>_SP</td><td>space</td><td></td></tr>\r\n",
    "<tr><td></td><td></td><td>NIL</td><td>missing tag</td><td></td></tr>\r\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e615655-c219-4adb-b3d2-0fda41507732",
   "metadata": {},
   "source": [
    "# Working with POS Tags\r\n",
    "In the English language, the same string of characters can have different meanings, even within the same sentence. For this reason, morphology is important. **spaCy** uses machine learning algorithms to best predict the use of a token in a sentence. Is *\"I read books on NLP\"* present or past tense? Is *wind* a verb or a noun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "decdec17-e528-4087-8643-18f3666b4eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read      VERB    VBP   verb, non-3rd person singular present\n",
      "read      VERB    VBD   verb, past tense\n",
      "reading   VERB    VBG   verb, gerund or present participle\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u'I read books on nlp')\n",
    "doc2 = nlp(u'I read a book on nlp')\n",
    "doc3 = nlp(u'I am reading a book on nlp')\n",
    "print(f'{doc1[1].text:{10}}{doc1[1].pos_:{8}}{doc1[1].tag_:{6}}{spacy.explain(doc1[1].tag_)}')\n",
    "print(f'{doc2[1].text:{10}}{doc2[1].pos_:{8}}{doc2[1].tag_:{6}}{spacy.explain(doc2[1].tag_)}')\n",
    "print(f'{doc3[2].text:{10}}{doc3[2].pos_:{8}}{doc3[2].tag_:{6}}{spacy.explain(doc3[2].tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eccb05-4854-4e5a-ad91-7964a8d61279",
   "metadata": {},
   "source": [
    "# Counting POS Tags\r\n",
    "The `Doc.count_by()` method accepts a specific token attribute as its argument, and returns a frequency count of the given attribute as a dictionary object. Keys in the dictionary are the integer values of the given attribute ID, and values are the frequency. Counts of zero are not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c884e53-df72-4059-9744-d60a0d9fde9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  84.\tADJ  \tadjective   :  3\n",
      "  85.\tADP  \tadposition  :  1\n",
      "  90.\tDET  \tdeterminer  :  2\n",
      "  92.\tNOUN \tnoun        :  2\n",
      " 100.\tVERB \tverb        :  1\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'The quick brown fox jumped over the lazy dog')\n",
    "pos_counts = doc.count_by(spacy.attrs.POS)\n",
    "for pos,freq in sorted(pos_counts.items()):\n",
    "    print(f'{pos:{4}}.\\t{doc.vocab[pos].text:{5}}\\t{spacy.explain(doc.vocab[pos].text):{10}}  :  {freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e96654a-2caa-45c2-a26a-fb17c46873cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1292078113972184607.\tIN   \tconjunction, subordinating or preposition  :  1\n",
      "10554686591937588953.\tJJ   \tadjective (English), other noun-modifier (Chinese)  :  3\n",
      "15267657372422890137.\tDT   \tdeterminer  :  2\n",
      "15308085513773655218.\tNN   \tnoun, singular or mass  :  2\n",
      "17109001835818727656.\tVBD  \tverb, past tense  :  1\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'The quick brown fox jumped over the lazy dog')\n",
    "tag_counts = doc.count_by(spacy.attrs.TAG)\n",
    "for tag,freq in sorted(tag_counts.items()):\n",
    "    print(f'{tag:{4}}.\\t{doc.vocab[tag].text:{5}}\\t{spacy.explain(doc.vocab[tag].text):{10}}  :  {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feacf664-6b69-4f15-8ffc-f179eae04f57",
   "metadata": {},
   "source": [
    "Why did the ID numbers get so big?\\\n",
    "In spaCy, certain text values are hardcoded into `Doc.vocab` and take up the first several hundred ID numbers. Strings like 'NOUN' and 'VERB' are used frequently by internal operations. Others, like fine-grained tags, are assigned hash values as needed.\n",
    "\n",
    "Why don't SPACE tags appear?\\\n",
    "In spaCy, only strings of spaces (two or more) are assigned tokens. Single spaces are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2982c987-be80-4d30-9008-2472ac522147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 402.\tamod \tadjectival modifier        :  3\n",
      "                 415.\tdet  \tdeterminer                 :  2\n",
      "                 429.\tnsubj\tnominal subject            :  1\n",
      "                 439.\tpobj \tobject of preposition      :  1\n",
      "                 443.\tprep \tprepositional modifier     :  1\n",
      " 8206900633647566924.\tROOT \troot                       :  1\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'The quick brown fox jumped over the lazy dog')\n",
    "dep_counts = doc.count_by(spacy.attrs.DEP)\n",
    "for dep,freq in sorted(dep_counts.items()):\n",
    "    print(f'{dep:{20}}.\\t{doc.vocab[dep].text:{5}}\\t{spacy.explain(doc.vocab[dep].text):{25}}  :  {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc5d397-942e-41a7-a079-b13d9ed262f1",
   "metadata": {},
   "source": [
    "# Visualizing Parts of Speech\r\n",
    "spaCy offers an outstanding visualizer called **displaCy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "272115b1-d46e-433d-bdd8-f1973c74fe50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"bb45d27de32f4c33acea7edf17e35ab1-0\" class=\"displacy\" width=\"1040\" height=\"302.0\" direction=\"ltr\" style=\"max-width: none; height: 302.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">quick</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">brown</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">fox</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">jumped</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"710\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"710\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">lazy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"212.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">dog</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bb45d27de32f4c33acea7edf17e35ab1-0-0\" stroke-width=\"2px\" d=\"M70,167.0 C70,2.0 380.0,2.0 380.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bb45d27de32f4c33acea7edf17e35ab1-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,169.0 L62,157.0 78,157.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bb45d27de32f4c33acea7edf17e35ab1-0-1\" stroke-width=\"2px\" d=\"M180,167.0 C180,57.0 375.0,57.0 375.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bb45d27de32f4c33acea7edf17e35ab1-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M180,169.0 L172,157.0 188,157.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bb45d27de32f4c33acea7edf17e35ab1-0-2\" stroke-width=\"2px\" d=\"M290,167.0 C290,112.0 370.0,112.0 370.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bb45d27de32f4c33acea7edf17e35ab1-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M290,169.0 L282,157.0 298,157.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bb45d27de32f4c33acea7edf17e35ab1-0-3\" stroke-width=\"2px\" d=\"M400,167.0 C400,112.0 480.0,112.0 480.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bb45d27de32f4c33acea7edf17e35ab1-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M400,169.0 L392,157.0 408,157.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bb45d27de32f4c33acea7edf17e35ab1-0-4\" stroke-width=\"2px\" d=\"M510,167.0 C510,112.0 590.0,112.0 590.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bb45d27de32f4c33acea7edf17e35ab1-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M590.0,169.0 L598.0,157.0 582.0,157.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bb45d27de32f4c33acea7edf17e35ab1-0-5\" stroke-width=\"2px\" d=\"M730,167.0 C730,57.0 925.0,57.0 925.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bb45d27de32f4c33acea7edf17e35ab1-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M730,169.0 L722,157.0 738,157.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bb45d27de32f4c33acea7edf17e35ab1-0-6\" stroke-width=\"2px\" d=\"M840,167.0 C840,112.0 920.0,112.0 920.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bb45d27de32f4c33acea7edf17e35ab1-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M840,169.0 L832,157.0 848,157.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-bb45d27de32f4c33acea7edf17e35ab1-0-7\" stroke-width=\"2px\" d=\"M620,167.0 C620,2.0 930.0,2.0 930.0,167.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-bb45d27de32f4c33acea7edf17e35ab1-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M930.0,169.0 L938.0,157.0 922.0,157.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from spacy import displacy\n",
    "doc = nlp(u'The quick brown fox jumped over the lazy dog')\n",
    "displacy.render(doc, style = 'dep', jupyter = True, options = {'distance' : 110})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35ac6842-166d-4682-b9e8-7b5e0a5b395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The        DET     det     determiner\n",
      "quick      ADJ     amod    adjectival modifier\n",
      "brown      ADJ     amod    adjectival modifier\n",
      "fox        NOUN    nsubj   nominal subject\n",
      "jumped     VERB    ROOT    root\n",
      "over       ADP     prep    prepositional modifier\n",
      "the        DET     det     determiner\n",
      "lazy       ADJ     amod    adjectival modifier\n",
      "dog        NOUN    pobj    object of preposition\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{7}} {token.dep_:{7}} {spacy.explain(token.dep_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34d1ce-3002-4954-a5f8-4aad57d67a4e",
   "metadata": {},
   "source": [
    "# Creating Visualizations Outside of Jupyter\n",
    "If you're using another Python IDE or writing a script, you can choose to have spaCy serve up HTML separately.\n",
    "\n",
    "Instead of `displacy.render()`, use `displacy.serve()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02f10686-eefe-405e-a9a5-ebf8f6f00752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displacy.serve(doc, style='dep', options={'distance': 110})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42592a5d-049d-4c6b-99bc-ed5fc40d102e",
   "metadata": {},
   "source": [
    "# Handling Large Text\n",
    "`displacy.serve()` accepts a single Doc or list of Doc objects. Since large texts are difficult to view in one line, you may want to pass a list of spans instead. Each span will appear on its own line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4b925e5-4356-4d5e-86dc-3e52ce1758f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"This is a sentence. This is another, possibly longer sentence.\")\n",
    "\n",
    "spans = list(doc2.sents)\n",
    "# displacy.serve(spans,  style = 'dep', options = {'distance' : 110})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf8216-4ecd-4946-bcbe-70cd4747624c",
   "metadata": {},
   "source": [
    "## Customizing the Appearance\r\n",
    "Besides setting the distance between tokens, you can pass other arguments to the `options` parameter:\r\n",
    "\r\n",
    "<table>\r\n",
    "<tr><th>NAME</th><th>TYPE</th><th>DESCRIPTION</th><th>DEFAULT</th></tr>\r\n",
    "<tr><td>`compact`</td><td>bool</td><td>\"Compact mode\" with square arrows that takes up less space.</td><td>`False`</td></tr>\r\n",
    "<tr><td>`color`</td><td>unicode</td><td>Text color (HEX, RGB or color names).</td><td>`#000000`</td></tr>\r\n",
    "<tr><td>`bg`</td><td>unicode</td><td>Background color (HEX, RGB or color names).</td><td>`#ffffff`</td></tr>\r\n",
    "<tr><td>`font`</td><td>unicode</td><td>Font name or font family for all text.</td><td>`Arial`</td></tr>\r\n",
    "</table>\r\n",
    "\r\n",
    "For a full list of options visit https://spacy.io/api/top-level#displacy_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91b01146-5076-4f4e-aff8-d209e49b7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'distance': 110, 'compact': 'True', 'color': 'yellow', 'bg': '#09a3d5', 'font': 'Times'}\n",
    "\n",
    "# displacy.serve(doc, style='dep', options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05942f73-2d03-4287-8a5e-786aff72c194",
   "metadata": {},
   "source": [
    "Great! Now you should be familiar with visualizing spaCy's dependency parse. For more info on **displaCy** visit https://spacy.io/usage/visualizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3625d-7474-4d49-ae63-cb2b5c095bae",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "spaCy has a NER pipeline component that identifies token spans fitting a predetermind=ed set of named entities. These are available as the `ents` property os a `doc` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2513daf2-adbf-48ae-9ab7-e200c14da67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington D.C.     -GPE                 -Countries, cities, states\n",
      "next May            -DATE                -Absolute or relative dates or periods\n",
      "the Washington Monument-ORG                 -Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(f'{ent.text:{20}}-{ent.label_:{20}}-{str(spacy.explain(ent.label_))}')\n",
    "    else:\n",
    "        print('No Named Entity Found')\n",
    "doc = nlp(u'May I go to Washington D.C. next May to see the Washington Monument?')\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e800d-7b2c-478f-8b8e-bc128dfe26ca",
   "metadata": {},
   "source": [
    "# NER Tags\r\n",
    "Tags are accessible through the `.label_` property of an entity.\r\n",
    "<table>\r\n",
    "<tr><th>TYPE</th><th>DESCRIPTION</th><th>EXAMPLE</th></tr>\r\n",
    "<tr><td>`PERSON`</td><td>People, including fictional.</td><td>*Fred Flintstone*</td></tr>\r\n",
    "<tr><td>`NORP`</td><td>Nationalities or religious or political groups.</td><td>*The Republican Party*</td></tr>\r\n",
    "<tr><td>`FAC`</td><td>Buildings, airports, highways, bridges, etc.</td><td>*Logan International Airport, The Golden Gate*</td></tr>\r\n",
    "<tr><td>`ORG`</td><td>Companies, agencies, institutions, etc.</td><td>*Microsoft, FBI, MIT*</td></tr>\r\n",
    "<tr><td>`GPE`</td><td>Countries, cities, states.</td><td>*France, UAR, Chicago, Idaho*</td></tr>\r\n",
    "<tr><td>`LOC`</td><td>Non-GPE locations, mountain ranges, bodies of water.</td><td>*Europe, Nile River, Midwest*</td></tr>\r\n",
    "<tr><td>`PRODUCT`</td><td>Objects, vehicles, foods, etc. (Not services.)</td><td>*Formula 1*</td></tr>\r\n",
    "<tr><td>`EVENT`</td><td>Named hurricanes, battles, wars, sports events, etc.</td><td>*Olympic Games*</td></tr>\r\n",
    "<tr><td>`WORK_OF_ART`</td><td>Titles of books, songs, etc.</td><td>*The Mona Lisa*</td></tr>\r\n",
    "<tr><td>`LAW`</td><td>Named documents made into laws.</td><td>*Roe v. Wade*</td></tr>\r\n",
    "<tr><td>`LANGUAGE`</td><td>Any named language.</td><td>*English*</td></tr>\r\n",
    "<tr><td>`DATE`</td><td>Absolute or relative dates or periods.</td><td>*20 July 1969*</td></tr>\r\n",
    "<tr><td>`TIME`</td><td>Times smaller than a day.</td><td>*Four hours*</td></tr>\r\n",
    "<tr><td>`PERCENT`</td><td>Percentage, including \"%\".</td><td>*Eighty percent*</td></tr>\r\n",
    "<tr><td>`MONEY`</td><td>Monetary values, including unit.</td><td>*Twenty Cents*</td></tr>\r\n",
    "<tr><td>`QUANTITY`</td><td>Measurements, as of weight or distance.</td><td>*Several kilometers, 55kg*</td></tr>\r\n",
    "<tr><td>`ORDINAL`</td><td>\"first\", \"second\", etc.</td><td>*9th, Ninth*</td></tr>\r\n",
    "<tr><td>`CARDINAL`</td><td>Numerals that do not fall under another type.</td><td>*2, Two, Fifty-two*</td></tr>\r\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790c1b4-85b7-4774-a942-799e60c7ee31",
   "metadata": {},
   "source": [
    "## Adding a Named Entity to a Span\r\n",
    "Normally we would have spaCy build a library of named entities by training it on several samples of text.<br>In this case, we only want to add one value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a2ad607-d0b0-4e86-8dd6-3b738cd86b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.K.                -GPE                 -Countries, cities, states\n",
      "$6 million          -MONEY               -Monetary values, including unit\n",
      "\n",
      "Tesla               -ORG                 -Companies, agencies, institutions, etc.\n",
      "U.K.                -GPE                 -Countries, cities, states\n",
      "$6 million          -MONEY               -Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Tesla to build a U.K. factory for $6 million')\n",
    "show_ents(doc)\n",
    "print()\n",
    "from spacy.tokens import Span\n",
    "ORG = doc.vocab.strings[u'ORG']\n",
    "new_ent = Span(doc, 0, 1, label = ORG)\n",
    "doc.ents = list(doc.ents) +[new_ent]\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba38c02-3845-4ebc-a543-5b7c1c5adf15",
   "metadata": {},
   "source": [
    "In the code above, the arguments passed to `Span()` are:\r\n",
    "-  `doc` - the name of the Doc object\r\n",
    "-  `0` - the *start* index position of the span\r\n",
    "-  `1` - the *stop* index position (exclusive)\r\n",
    "-  `label=ORG` - the label assigned to our entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b66f3-3b75-45f3-bca1-39af6a62b868",
   "metadata": {},
   "source": [
    "## Adding Named Entities to All Matching Spans\r\n",
    "What if we want to tag *all* occurrences of \"Tesla\"? In this section we show how to use the PhraseMatcher to identify a series of spans in the Doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08eea442-54b1-4531-b926-89cfccadc71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first               -ORDINAL             -\"first\", \"second\", etc.\n",
      "\n",
      " [(2689272359382549672, 7, 9), (2689272359382549672, 14, 16)] \n",
      "\n",
      "vaccum cleaner      -PRODUCT             -Objects, vehicles, foods, etc. (not services)\n",
      "vaccum cleaner      -PRODUCT             -Objects, vehicles, foods, etc. (not services)\n",
      "first               -ORDINAL             -\"first\", \"second\", etc.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Our company plans to introduce a new vaccum cleaner.'\n",
    "         u'If successful, the vaccum cleaner wil be our first product')\n",
    "show_ents(doc)\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrase_list = ['vaccum cleaner', 'vaccum-cleaner']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add('newproduct', phrase_patterns)\n",
    "matches = matcher(doc)\n",
    "print('\\n',matches,'\\n')\n",
    "\n",
    "# from spacy.tokens import Span\n",
    "PROD = doc.vocab.strings[u'PRODUCT']\n",
    "new_ents = [Span(doc, match[1], match[2], label = PROD) for match in matches]\n",
    "doc.ents = list(doc.ents) + new_ents\n",
    "show_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "660e7cbd-cfe7-46e7-bc62-1411961283db",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove('newproduct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3502348-9546-40d3-83e8-e3e1dc7dccac",
   "metadata": {},
   "source": [
    "## Counting Entities\r\n",
    "While spaCy may not have a built-in tool for counting entities, we can pass a conditional statement into a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f3e1434-527e-4c1d-bab3-fbfd08b5d867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.50               -MONEY               -Monetary values, including unit\n",
      "five dollars        -MONEY               -Monetary values, including unit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Originally priced at $29.50, the sewater was marked down to five dollars.')\n",
    "show_ents(doc)\n",
    "#print()\n",
    "len([ent for ent in doc.ents if ent.label_ == 'MONEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bee4b-e842-44fb-bf47-478a3f0c8d99",
   "metadata": {},
   "source": [
    "# Problem with line breaks\n",
    "Sometimes there arises a issue where linebreaks are interpreted as `GPE` entities\\\n",
    "However that can be fixed by adding a `NLP pipeline` to remove the white space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0d8ca9b-ea73-45d5-b40a-1b8529fc661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'remove_whitespace_entities']\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "@Language.component(\"remove_whitespace_entities\")\n",
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
    "    return doc\n",
    "nlp.add_pipe(\"remove_whitespace_entities\", after = 'ner')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9f5e3-21bb-4600-9841-b3f0618aefc9",
   "metadata": {},
   "source": [
    "# Remove a pipeline\n",
    "Similarly to remove a pipeline from the NLP we use the funtion `nlp.remove_pipe(\"<nameOfThePipeline>\")`\n",
    "\n",
    "To disable a pipeline at loading we use `nlp = spacy.load(\"en_core_web_sm\", enable = [\"tok2vec\", \"tagger\"], disable = [\"ner\"])`\n",
    "\n",
    "To exclude a pipeline at loading we use `nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])`\r\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90075c85-a09a-4e9f-a6bb-23383900a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "nlp.remove_pipe('remove_whitespace_entities')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1a869-7c8e-412f-8b46-c9793bd28038",
   "metadata": {},
   "source": [
    "# Noun Chuncks\n",
    "`doc.noun_chunks` are basic noun phrases : token spans that include the noun and words describing the noun. Noun chuncks cannot be nested, cannot overlap, abd do not involve prepositional phrases or relative clauses.\\ Where `doc.ents` rely on the *ner* pipeline component, `doc.noun_chuncks` are provided by the *parser*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2888a-a650-4a14-9334-47bcb786f699",
   "metadata": {},
   "source": [
    "# `noun_chunks` components:\n",
    "<table>\n",
    "<tr><td>`.text`</td><td>The original noun chunk text.</td></tr>\n",
    "<tr><td>`.root.text`</td><td>The original text of the word connecting the noun chunk to the rest of the parse.</td></tr>\n",
    "<tr><td>`.root.dep_`</td><td>Dependency relation connecting the root to its head.</td></tr>\n",
    "<tr><td>`.root.head.text`</td><td>The text of the root token's head.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1be2fbc5-7af7-4345-8e99-b88f454c69b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars          cars           nsubj     shift\n",
      "insurance liability      liability      dobj      shift\n",
      "manufacturers            manufacturers  pobj      toward\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f'{chunk.text:{25}}{chunk.root.text:{15}}{chunk.root.dep_:{10}}{chunk.root.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219aee6f-a6af-47d2-884d-75bba9b0f934",
   "metadata": {},
   "source": [
    "# `Doc.noun_chunks` is a  generator function\r\n",
    "Previously we mentioned that `Doc` objects do not retain a list of sentences, but they're available through the `Doc.sents` generator.<br>It's the same with `Doc.noun_chunks` - lists can be created if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e011dc0-0dfc-449c-acce-7e602dfe28db",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoun_chunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "#len(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab84eb65-50de-42ac-93ab-149d2c6c37a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(doc.noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd68faf-76ca-473b-86b6-18730b8fd271",
   "metadata": {},
   "source": [
    "# Visualizing named entities\n",
    "Like visualizing dependencies with `style='dep'` *displacy* also offers a `style='ent'` visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7536762a-0abd-4913-b55d-84d8ec0becab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quater\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nearly 20 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". By contrast, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sony\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    only 7 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " Walkman music players. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'Over the last quater Apple sold nearly 20 thousand iPods for a profit of $6 million. '\n",
    "         u'By contrast, Sony sold only 7 thousand Walkman music players. ')\n",
    "displacy.render(doc, style = 'ent', jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079cde8-93c4-4b63-85bd-68e5542beafe",
   "metadata": {},
   "source": [
    "# Viewing Sentences Line by Line\n",
    "Unlike displaCy dependency parse, the NER viewer has to take in a doc object with `ents` attribute. For this reason, we can't just pass a list of spans to `.render()`, we have to create a new doc from each `span.text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdec7d56-14ba-4287-acfa-7202ebecbd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nearly 20 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\spacy\\displacy\\__init__.py:213: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">By contrast, the vendor near my house sold a lot of lemonade.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.'\n",
    "         u'By contrast, the vendor near my house sold a lot of lemonade.')\n",
    "for sent in doc.sents:\n",
    "    displacy.render(nlp(sent.text), style = 'ent', jupyter = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3e21f-3453-4cbd-af89-c042ff2a7dbd",
   "metadata": {},
   "source": [
    "If a span does not contain any entities, displaCy will issue a harless warning. that can be avoided by an additional bit of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4bedbe55-b476-45e2-af13-7c9cf5a825bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the last quarter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    nearly 20 thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $6 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "By contrast, the vendor near my house sold a lot of lemonade.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    docx = nlp(sent.text)\n",
    "    if docx.ents:\n",
    "        displacy.render(docx, style='ent', jupyter=True)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print(docx.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a721f7-faf4-4765-9b6f-ab7fde090cd6",
   "metadata": {},
   "source": [
    "# Viewing Specific Entities \n",
    "You can pass a list of entity types to restrict the visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6b0044a-0d79-4488-9cf2-c2340b775636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over the last quarter \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold nearly 20 thousand \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of $6 million.By contrast, the vendor near my house sold a lot of lemonade.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = {'ents' : ['ORG', 'PRODUCT']}\n",
    "displacy.render(doc, style = 'ent', jupyter = True, options = opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae1813-9e9d-41b1-b57c-3f0266c96e34",
   "metadata": {},
   "source": [
    "# Customimizing colors and effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef2ebd6a-5b86-4bcb-985d-98f02e369eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over the last quarter \n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " sold nearly 20 thousand \n",
       "<mark class=\"entity\" style=\"background: radial-gradient(yellow, green); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPods\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " for a profit of $6 million.By contrast, the vendor near my house sold a lot of lemonade.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)', 'PRODUCT': 'radial-gradient(yellow, green)'}\n",
    "options = {'ents': ['ORG', 'PRODUCT'], 'colors':colors}\n",
    "displacy.render(doc, style = 'ent', jupyter = True, options = options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3845e1-bc41-435e-96f9-642c18d5e666",
   "metadata": {},
   "source": [
    "# Creating Visualizations Outside of Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86720e37-ba27-42ef-ad75-3533e4c8eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# displacy.serve(doc, style = 'ent', options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a11f53-4924-4d2c-a002-f61cef357624",
   "metadata": {},
   "source": [
    "# Sentence Segmentation\n",
    "In this section we'll use sentence segmentation n set our own segmentation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa1d9ba6-5708-4d89-9624-85409bd710f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is the second one.\n",
      "This is the last sentence\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'This is the first sentence. This is the second one. This is the last sentence')\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc82a3-98a5-40ef-bb47-e4272146339a",
   "metadata": {},
   "source": [
    "# `Doc.sents` is a generator\r\n",
    "It is important to note that `doc.sents` is a *generator*. That is, a Doc is not segmented until `doc.sents` is called. This means that, where you could print the second Doc token with `print(doc[1])`, you can't call the \"second Doc sentence\" with `print(doc.sents[1])`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f67c6680-c67f-4dd6-b262-e7a03613b789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Management\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(doc[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(doc[1])\n",
    "#print(doc.sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bacf6997-f54d-414a-8dc6-4d7ae3e18b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is the second one.\n",
      "This is the last sentence\n"
     ]
    }
   ],
   "source": [
    "doc_sents = [sent for sent in doc.sents]\n",
    "print(f'{doc_sents[0]}\\n{doc_sents[1]}\\n{doc_sents[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba344937-673a-468d-b3ba-ab8b0782ca56",
   "metadata": {},
   "source": [
    "# `sents` are Spans\r\n",
    "At first glance it looks like each `sent` contains text from the original Doc object. In fact they're just Spans with start and end token pointers.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8dd213b-7e12-414e-8a61-1f1538c1ead3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41354bcb-dcf9-4161-a9cf-cb5d0cefc8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 12\n"
     ]
    }
   ],
   "source": [
    "print(doc_sents[1].start, doc_sents[1].end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb2026-c9b4-44bc-ad4a-11e2b700142e",
   "metadata": {},
   "source": [
    "# Adding Rules\r\n",
    "spaCy's built-in `sentencizer` relies on the dependency parse and end-of-sentence punctuation to determine segmentation rules. We can add rules of our own, but they have to be added *before* the creation of the Doc object, as that is where the parsing of segment start tokens happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37cb1493-ad29-46ad-95b2-530aabc83d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True  This\n",
      "False  is\n",
      "False  a\n",
      "False  sentence\n",
      "False  .\n",
      "True  This\n",
      "False  is\n",
      "False  a\n",
      "False  sentence\n",
      "False  .\n",
      "True  This\n",
      "False  is\n",
      "False  a\n",
      "False  sentence\n",
      "False  .\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u'This is a sentence. This is a sentence. This is a sentence.')\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.is_sent_start, ' '+token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f700016-4c32-4e87-85db-35acbc123c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Management is doing things right; leadership is doing the right things.\"\n",
      "-Peter Drucker\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592467e-2558-4dd7-ba02-74202009ba9e",
   "metadata": {},
   "source": [
    "Now let's add a new rule to form sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d7efc900-5494-454f-b963-50c3ec4f5abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'set_custom_boundaries', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'] \n",
      "\n",
      "\"Management is doing things right; leadership is doing the right things.\"\n",
      "-Peter Drucker\n",
      "\n",
      "\"Management is doing things right;\n",
      "leadership is doing the right things.\"\n",
      "-Peter Drucker\n",
      "\n",
      " ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "@Language.component('set_custom_boundaries')\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == ';':\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "nlp.add_pipe('set_custom_boundaries', before = 'parser')\n",
    "print(nlp.pipe_names,'\\n')\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "print()\n",
    "doc1 = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')   \n",
    "for sent in doc1.sents:\n",
    "    print(sent)\n",
    "    \n",
    "nlp.remove_pipe('set_custom_boundaries')\n",
    "print('\\n',nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0623f81c-9bfd-4e59-b925-5aa1c82728da",
   "metadata": {},
   "source": [
    "Older doc objects created do not show any change\\\n",
    "# Why not change the token directly\n",
    "We dont directly assign `.is_sent_start` value to `True` as spaCy refuses to change the tag after the document is parsed to prevent inconsistencies in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3c0e631-e9f8-4d8d-a0d1-30b6a7f2da90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leadership\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E043] Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(doc[\u001b[38;5;241m7\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_sent_start\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\spacy\\tokens\\token.pyx:528\u001b[0m, in \u001b[0;36mspacy.tokens.token.Token.is_sent_start.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E043] Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state."
     ]
    }
   ],
   "source": [
    "print(doc[7])\n",
    "#doc[7].is_sent_start = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea406d8-f336-44dc-8bd4-55333b2b0f10",
   "metadata": {},
   "source": [
    "# Changing the rules\n",
    "In some cases we want to replace spaCy's default sentencizer with our own set of rules.\\\n",
    "We can change the rules as per our wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "54d1b170-feeb-4227-a682-60dcdabffc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sentence', '.']\n",
      "['This', 'is', 'another', '.', '\\n\\n']\n",
      "['thus', 'is', 'the', '\\n', 'third', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "mystring = u'This is a sentence. This is another.\\n\\nthus is the \\nthird sentence.'\n",
    "doc = nlp(mystring)\n",
    "for sent in doc.sents:\n",
    "    print([token.text for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "744b85e9-b262-4e75-9e67-af41f3428fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('split_on_newlist', <function __main__.split_on_newlist(doc)>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.remove_pipe('split_on_newlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "be182236-6349-42db-93a5-15c36b630b07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E005] Pipeline component 'split_on_newlist' returned <class 'generator'> instead of a Doc. If you're using a custom component, maybe you forgot to return the processed Doc?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[0;32m     15\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_on_newlist\u001b[39m\u001b[38;5;124m'\u001b[39m, before \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmystring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m([token\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sent])\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\spacy\\language.py:1056\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1054\u001b[0m         error_handler(name, proc, [doc], e)\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, Doc):\n\u001b[1;32m-> 1056\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE005\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, returned_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc)))\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: [E005] Pipeline component 'split_on_newlist' returned <class 'generator'> instead of a Doc. If you're using a custom component, maybe you forgot to return the processed Doc?"
     ]
    }
   ],
   "source": [
    "@Language.component('split_on_newlist')\n",
    "def split_on_newlist(doc):\n",
    "    start = 0\n",
    "    seen_newline = False\n",
    "    for word in doc:\n",
    "        if word.text.startswith('\\n'):\n",
    "            doc[]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe('split_on_newlist', before = 'parser')\n",
    "doc = nlp(mystring)\n",
    "for sent in doc.sents:\n",
    "    print([token.text for token in sent])\n",
    "nlp.remove_pipe('split_on_newlist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
